# -*- coding: utf-8 -*-
"""proc_data_int_ven.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18SBk7qJiMOLLutbG-Ncx7nDb04II09wT
"""

import pandas as pd
import boto3
import os

# ========================================
# Leitura dos dados
# ========================================
def ler_dados(caminhos: dict) -> dict:
    """Lê arquivos CSV a partir de um dicionário de caminhos e retorna um dicionário de DataFrames."""
    return {chave: pd.read_csv(caminho) for chave, caminho in caminhos.items()}

# ========================================
# Análise rápida das estruturas
# ========================================
def analisar_dados(dfs: dict):
    """Exibe as colunas presentes em cada DataFrame e percentual de ausentes."""
    for nome, df in dfs.items():
        print(f"\nAnalisando {nome.upper()}:")
        print(f" - Colunas: {df.columns.tolist()}")
        print(f" - Dimensões: {df.shape[0]} linhas x {df.shape[1]} colunas")
        print(f" - Tipos de dados:\n{df.dtypes}")
        na = df.isna().sum().sum()
        total = df.size
        perc = (na / total) * 100
        print(f" - Percentual de valores ausentes: {perc:.2f}%")

# ========================================
# Processamento: união e seleção de colunas
# ========================================
def processar_dados(dfs: dict) -> pd.DataFrame:
    df_interacoes = dfs["df_interacoes"]
    df_vendas = dfs["df_vendas"]

    df_merged = pd.merge(df_interacoes, df_vendas, on="id_cliente", how="left")

    colunas_desejadas = [
        'id_cliente',
        'data_interacao',
        'motivo',
        'tempo_resposta_segundos',
        'canal_venda',
        'tipo_produto',
        'valor_premio',
        'status',
        'data_venda',
        'id_venda'
    ]

    colunas_existentes = [col for col in colunas_desejadas if col in df_merged.columns]
    df_final = df_merged[colunas_existentes]

    return df_final

# ========================================
# Armazenamento no S3 com segurança
# ========================================
def armazenamento_dados(df_final: pd.DataFrame):
    """Salva o DataFrame como Parquet localmente e envia para o S3."""

    # Salva como Parquet localmente
    file_path = 'dataset_unificado.parquet'
    df_final.to_parquet(file_path, index=False)

    # Acessa credenciais via variáveis de ambiente
    s3 = boto3.client(
        's3',
        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),
        region_name='us-east-1'
    )

    # Define bucket e caminho no S3
    bucket_name = 'brasilseg-analise-performance-comercial'
    s3_key = 'interacoes-vendas/data_processed/dataset_unificado.parquet'

    # Faz upload
    s3.upload_file(file_path, bucket_name, s3_key)
    print("Upload concluído com sucesso para o S3!")

# ========================================
# Execução principal
# ========================================
def execucao_principal():
    """Executa o pipeline completo de leitura, análise, união e envio ao S3."""
    caminhos = {
        "df_interacoes": "/content/drive/MyDrive/Consutorias/Brasil Seg/data_raw/interacoes_callcenter.csv",
        "df_vendas": "/content/drive/MyDrive/Consutorias/Brasil Seg/data_raw/vendas_seguros.csv"
    }

    dfs = ler_dados(caminhos)
    analisar_dados(dfs)

    df_final = processar_dados(dfs)
    armazenamento_dados(df_final)

    return df_final

# ========================================
# Execução
# ========================================
if __name__ == "__main__":
    df_resultado = execucao_principal()